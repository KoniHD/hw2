{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoniHD/hw2/blob/main/notebooks/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ps_Sgfwa2jw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUK1XgCa2jz"
      },
      "source": [
        "## Clone Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSDhZpmd9iYR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    if not os.path.exists(\"hw2/pyproject.toml\"):\n",
        "        print(\"Repo doesn't exist yet. Cloning from github ...\")\n",
        "        !git clone --quiet --depth 1 https://github.com/KoniHD/hw2.git\n",
        "        os.chdir(\"hw2\")\n",
        "        !uv pip install --quiet -r requirements.txt --system\n",
        "        os.chdir(\"..\")\n",
        "        print(\"Cloned Repo successfully!\")\n",
        "        os.kill(os.getpid(), 9)  # Restart kernel to make modules available\n",
        "else:\n",
        "    os.chdir(\"..\")\n",
        "    !uv sync"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Requires a **restart** the Notebook to make new pip installs active."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\n",
        "    f\"{'Success!' if os.path.exists('hw2/pyproject.toml') else 'Failed to clone Repo!'}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Di6-eua2jy"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rM1U4K1_a2jy"
      },
      "outputs": [],
      "source": [
        "# Fetch data\n",
        "if not os.path.exists(\"data\"):\n",
        "    !mkdir -p data\n",
        "    !wget -q -P data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n",
        "    !unzip -q -n data/train-test-data.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pirgyzOY9soh"
      },
      "source": [
        "## Imports libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL3J8GNs8zZS"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import upload_file\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import (\n",
        "    BackboneFinetuning,\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        ")\n",
        "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from data.custom_transforms import (\n",
        "    Normalize,\n",
        "    RandomCrop,\n",
        "    Rescale,\n",
        "    ToTensor,\n",
        ")\n",
        "from data.facial_keypoints_dataset import (\n",
        "    FacialKeypointsDataset,\n",
        "    FacialKeypointsHeatmapDataset,\n",
        ")\n",
        "from keypoint_task import KeypointDetection\n",
        "from models.resnet18 import ResNetKeypointDetector\n",
        "from models.simple_cnn import Simple_CNN\n",
        "from models.unet import UNetKeypointDetector\n",
        "from utils import visualize_batch, visualize_heatmaps, visualize_loss_curve\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    from google.colab import userdata\n",
        "\n",
        "    hf_token = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GicqPL44y7qW"
      },
      "source": [
        "## Set Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UVi7aUiy7qZ"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Data\n",
        "    \"batch_size\": 16,\n",
        "    \"img_size\": 224,\n",
        "    # Model\n",
        "    \"out_dim\": 136,\n",
        "    \"activation\": \"relu\",\n",
        "    \"dropout\": 0.3,\n",
        "    \"batch_norm\": True,\n",
        "    # Training\n",
        "    \"lr\": 4e-3,\n",
        "    \"max_epochs\": 30,\n",
        "    \"criterion\": \"mse\",\n",
        "    \"random_seed\": 42,\n",
        "    \"patience\": 5,\n",
        "    \"optimizer\": \"adam\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbZxP8lka2j1"
      },
      "source": [
        "## Load Data and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwfRjCZra2j2"
      },
      "outputs": [],
      "source": [
        "seed_everything(\n",
        "    config[\"random_seed\"], workers=True\n",
        ")  # Try to create deterministic results\n",
        "\n",
        "# defining the data_transform using transforms.Compose([all tx's, . , .])\n",
        "# order matters! i.e. rescaling should come before a smaller crop\n",
        "data_transform = transforms.Compose(\n",
        "    [Rescale(250), RandomCrop(config[\"img_size\"]), Normalize(), ToTensor()]\n",
        ")\n",
        "\n",
        "training_keypoints_csv_path = os.path.join(\"data\", \"training_frames_keypoints.csv\")\n",
        "training_data_dir = os.path.join(\"data\", \"training\")\n",
        "test_keypoints_csv_path = os.path.join(\"data\", \"test_frames_keypoints.csv\")\n",
        "test_data_dir = os.path.join(\"data\", \"test\")\n",
        "\n",
        "\n",
        "# create the transformed dataset\n",
        "transformed_dataset = FacialKeypointsDataset(\n",
        "    csv_file=training_keypoints_csv_path,\n",
        "    root_dir=training_data_dir,\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "# load training data in batches\n",
        "train_loader = DataLoader(\n",
        "    transformed_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=2\n",
        ")  # num_workers changed to Colab recommended number\n",
        "\n",
        "# creating the test dataset\n",
        "test_dataset = FacialKeypointsDataset(\n",
        "    csv_file=test_keypoints_csv_path, root_dir=test_data_dir, transform=data_transform\n",
        ")\n",
        "\n",
        "# loading test data in batches\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=2\n",
        ")  # num_workers changed to Colab recommended number, shuffle changed to False\n",
        "\n",
        "test_batch = next(iter(test_loader))\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    sample = data\n",
        "    image = sample[\"image\"][0]\n",
        "    keypoints = sample[\"keypoints\"][0]\n",
        "    _, h, w = image.shape\n",
        "    # plot the image black and white\n",
        "    plt.imshow(image.numpy().transpose(1, 2, 0), cmap=\"gray\")\n",
        "    plt.scatter(\n",
        "        keypoints[:, 0] * (w / 2) + (w / 2),\n",
        "        keypoints[:, 1] * (h / 2) + (h / 2),\n",
        "        c=\"r\",\n",
        "        s=20,\n",
        "    )\n",
        "    plt.show()\n",
        "    print(f\"Image min/max:   {image.min():.4f} / {image.max():.4f}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_w-xZmPB8a"
      },
      "source": [
        "# Data Exploration & Sanity Checks\n",
        "\n",
        "Observe basic dataset characteristics and sanity check via **model overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmhi2zWAPIKP"
      },
      "outputs": [],
      "source": [
        "print(\"===Metrics of first batch===\")\n",
        "batch = next(iter(train_loader))\n",
        "images, keypoints = batch[\"image\"], batch[\"keypoints\"]\n",
        "\n",
        "print(f\"Image shape:\\t\\t{images.shape}\")\n",
        "print(\n",
        "    f\"Image min/max:\\t\\t{images.min():.4f} / {images.max():.4f}\\t\\twithin [0, 1]: {(-0 <= images.min().round(decimals=1) and images.max().round(decimals=1) <= 1)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Keypoints min/max:\\t{keypoints.min():.4f} / {keypoints.max():.4f}\\twithin [-1, 1]: {(-1 <= keypoints.min().round(decimals=1) and keypoints.max().round(decimals=1) <= 1)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Direct Coordinate Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optional:** Load existing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_cnn = Simple_CNN.from_pretrained(\"KoniHD/Simple_CNN\")\n",
        "keypoint_task = KeypointDetection(\n",
        "    model=simple_cnn,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    activation=config[\"activation\"],\n",
        "    droput=config[\"dropout\"],\n",
        "    batch_norm=config[\"batch_norm\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overfitting\n",
        "\n",
        "Evaluation whether the model architecture is sufficient.\n",
        "(Can be skipped when loading existing model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSjMHWkcy7qq"
      },
      "outputs": [],
      "source": [
        "default_exp_dir = \"exp/simple_cnn/\"\n",
        "\n",
        "# Model\n",
        "simple_cnn = Simple_CNN(\n",
        "    out_dim=config[\"out_dim\"],\n",
        "    activation=config[\"activation\"],\n",
        "    dropout=0.0,  # No dropout when overfitting\n",
        "    batch_norm=False,  # No batch_norm while overfitting\n",
        ")\n",
        "\n",
        "# Lightning Wrapper\n",
        "keypoint_task = KeypointDetection(\n",
        "    model=simple_cnn,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    activation=config[\"activation\"],\n",
        "    droput=0.0,\n",
        "    batch_norm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=200,\n",
        "    accelerator=\"auto\",\n",
        "    deterministic=\"warn\",\n",
        "    logger=False,\n",
        "    default_root_dir=default_exp_dir,\n",
        "    detect_anomaly=True,\n",
        "    overfit_batches=1,\n",
        "    enable_autolog_hparams=False,\n",
        "    enable_checkpointing=False,\n",
        ")\n",
        "trainer.fit(keypoint_task, train_dataloaders=train_loader)\n",
        "\n",
        "metrics = trainer.callback_metrics\n",
        "print(f\"\\n\\n=============\\nFinal train loss: {metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_aqeW8chGh-"
      },
      "source": [
        "Visualize overfitting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ9aXOqUehnU"
      },
      "outputs": [],
      "source": [
        "visualize_batch(model=simple_cnn, batch=test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrZ6RSxAhWdp"
      },
      "source": [
        "### Real training\n",
        "\n",
        "Start with clean model. (Can be ignored when loading exisiting model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "simple_cnn = Simple_CNN(\n",
        "    out_dim=config[\"out_dim\"],\n",
        "    activation=config[\"activation\"],\n",
        "    dropout=config[\"dropout\"],\n",
        "    batch_norm=config[\"batch_norm\"],\n",
        ")\n",
        "\n",
        "# Lightning Wrapper\n",
        "keypoint_task = KeypointDetection(\n",
        "    model=simple_cnn,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    activation=config[\"activation\"],\n",
        "    droput=config[\"dropout\"],\n",
        "    batch_norm=config[\"batch_norm\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SuJ3fC667--P"
      },
      "outputs": [],
      "source": [
        "version = 0\n",
        "\n",
        "# keypoint_task = torch.compile(keypoint_task, mode=\"reduce-overhead\") # Can only be used when Tenforboard.log_graph=False\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=default_exp_dir + f\"version_{version}\",\n",
        "    filename=\"simple-cnn\",\n",
        "    monitor=\"train_loss\",  # Do not monitor on test set!!\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        "    save_last=True,\n",
        "    save_weights_only=True,\n",
        "    enable_version_counter=True,\n",
        ")\n",
        "\n",
        "earlystopping_callback = EarlyStopping(\n",
        "    monitor=\"train_loss\",  # Do not monitor on test set!!\n",
        "    patience=config[\"patience\"],\n",
        "    mode=\"min\",\n",
        "    min_delta=0.001,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    logger=[\n",
        "        TensorBoardLogger(\n",
        "            default_exp_dir,\n",
        "            name=\"\",  # necessary to save logs to the same directory as CSVLogger\n",
        "            version=f\"version_{version}\",\n",
        "            log_graph=True,\n",
        "            default_hp_metric=False,\n",
        "        ),\n",
        "        CSVLogger(default_exp_dir, name=\"\", version=f\"version_{version}\"),\n",
        "    ],\n",
        "    max_epochs=config[\"max_epochs\"],\n",
        "    callbacks=[checkpoint_callback, earlystopping_callback],\n",
        "    deterministic=\"warn\",  # Used for attempted reporduceability\n",
        "    default_root_dir=default_exp_dir,\n",
        "    num_sanity_val_steps=0,\n",
        "    enable_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer.fit(keypoint_task, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "\n",
        "keypoint_task = KeypointDetection.load_from_checkpoint(\n",
        "    checkpoint_callback.best_model_path, weights_only=True, model=simple_cnn\n",
        ")\n",
        "\n",
        "metrics = trainer.callback_metrics\n",
        "print(\n",
        "    f\"\\n\\n=============\\nFinal train loss: {metrics['train_loss']:.4f}\\nFinal val loss: {metrics['val_loss']:.4f}\"\n",
        ")\n",
        "\n",
        "visualize_batch(model=simple_cnn, batch=test_batch)\n",
        "visualize_loss_curve(\n",
        "    logs=f\"{default_exp_dir}version_{version}/metrics.csv\",\n",
        "    title=f\"Part 1: Simple CNN Training Curve Version {version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z75HW8OkhcnD"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ld5MBWE7wp8c"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {default_exp_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optional:** Save model weights and training log to huggingface for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTaGYyrB-nzD"
      },
      "outputs": [],
      "source": [
        "model_to_save = getattr(\n",
        "    simple_cnn, \"_orig_mod\", simple_cnn\n",
        ")  # make sure to upload the non-compiled model\n",
        "model_to_save.push_to_hub(\n",
        "    \"KoniHD/Simple_CNN\",\n",
        "    config=config,\n",
        "    commit_message=f\"Training run version: {version}\",\n",
        "    private=True,\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "tfevents_file = glob.glob(f\"{default_exp_dir}version_{version}/*.tfevents.*\")[0]\n",
        "upload_file(\n",
        "    path_or_fileobj=tfevents_file,\n",
        "    path_in_repo=f\"logs/run{version + 1}/events.out.tfevents\",\n",
        "    repo_id=\"KoniHD/Simple_CNN\",\n",
        "    token=hf_token,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=f\"Logs from run no. {version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVtHhRjaa2j5"
      },
      "source": [
        "## Part 2: Transfer Learning for Keypoint Detection\n",
        "\n",
        "### Set new hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezzFlbK2a2j5"
      },
      "outputs": [],
      "source": [
        "default_exp_dir = \"exp/restnet/\"\n",
        "\n",
        "config = {\n",
        "    # Data\n",
        "    \"batch_size\": 16,\n",
        "    \"img_size\": 224,\n",
        "    # Model\n",
        "    \"grayScale\": True,\n",
        "    \"out_dim\": 136,\n",
        "    # Training\n",
        "    \"lr\": 4e-3,\n",
        "    \"max_epochs\": 50,\n",
        "    \"criterion\": \"mse\",\n",
        "    \"random_seed\": 42,\n",
        "    \"patience\": 10,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"pretrained_backbone\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tuning ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 0\n",
        "\n",
        "resnet18 = ResNetKeypointDetector(\n",
        "    out_dim=config[\"out_dim\"], grayScale=config[\"grayScale\"]\n",
        ")\n",
        "\n",
        "keypoint_task = KeypointDetection(\n",
        "    model=resnet18,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    pretrained_backbone=config[\"pretrained_backbone\"],\n",
        "    activation=\"relu\",  # Predefined by ResNet18 architecture\n",
        "    dropout=0.3,  # Predefined by ResNet18 architecture\n",
        "    batch_norm=True,  # Predefined by ResNet18 architecture\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=default_exp_dir + f\"version_{version}\",\n",
        "    filename=\"resnet18\",\n",
        "    monitor=\"train_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        "    save_last=True,\n",
        "    save_weights_only=True,\n",
        "    enable_version_counter=True,\n",
        ")\n",
        "\n",
        "earlystopping_callback = EarlyStopping(\n",
        "    monitor=\"train_loss\",\n",
        "    patience=config[\"patience\"],\n",
        "    mode=\"min\",\n",
        "    min_delta=0.001,\n",
        ")\n",
        "\n",
        "finetuning_callback = BackboneFinetuning(\n",
        "    unfreeze_backbone_at_epoch=8,\n",
        "    lambda_func=lambda epoch: 1.05,\n",
        "    backbone_initial_ratio_lr=0.1,\n",
        "    should_align=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    logger=[\n",
        "        TensorBoardLogger(\n",
        "            default_exp_dir,\n",
        "            name=\"\",  # necessary to save logs to the same directory as CSVLogger\n",
        "            version=f\"version_{version}\",\n",
        "            log_graph=True,\n",
        "            default_hp_metric=False,\n",
        "        ),\n",
        "        CSVLogger(default_exp_dir, name=\"\", version=f\"version_{version}\"),\n",
        "    ],\n",
        "    max_epochs=config[\"max_epochs\"],\n",
        "    callbacks=[checkpoint_callback, earlystopping_callback, finetuning_callback],\n",
        "    deterministic=\"warn\",\n",
        "    default_root_dir=default_exp_dir,\n",
        "    enable_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer.fit(keypoint_task, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "\n",
        "keypoint_task = KeypointDetection.load_from_checkpoint(\n",
        "    checkpoint_callback.best_model_path, weights_only=True, model=resnet18\n",
        ")\n",
        "\n",
        "metrics = trainer.callback_metrics\n",
        "print(\n",
        "    f\"\\n\\n=============\\nFinal train loss: {metrics['train_loss']:.4f}\\nFinal val loss: {metrics['val_loss']:.4f}\"\n",
        ")\n",
        "\n",
        "visualize_batch(model=resnet18, batch=test_batch)\n",
        "visualize_loss_curve(\n",
        "    logs=f\"{default_exp_dir}version_{version}/metrics.csv\",\n",
        "    title=f\"Part 2: ResNet18 Fine-tuning Training Curve Version {version}\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {default_exp_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optional:** Save results to huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_to_save = getattr(\n",
        "    resnet18, \"_orig_mod\", resnet18\n",
        ")  # make sure to upload the non-compiled model\n",
        "model_to_save.push_to_hub(\n",
        "    \"KoniHD/Fine-Tuned-ResNet18\",\n",
        "    config=config,\n",
        "    commit_message=f\"Training run version: {version}\",\n",
        "    private=True,\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "tfevents_file = glob.glob(f\"{default_exp_dir}version_{version}/*.tfevents.*\")[0]\n",
        "upload_file(\n",
        "    path_or_fileobj=tfevents_file,\n",
        "    path_in_repo=f\"logs/run{version + 1}/events.out.tfevents\",\n",
        "    repo_id=\"KoniHD/Fine-Tuned-ResNet18\",\n",
        "    token=hf_token,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=f\"Logs from run no. {version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rP0fd7Ea2j7"
      },
      "source": [
        "## Part 3: Heatmap-based Keypoint Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfszKqgaa2j7"
      },
      "outputs": [],
      "source": [
        "default_exp_dir = \"exp/unet/\"\n",
        "\n",
        "config = {\n",
        "    # Data\n",
        "    \"batch_size\": 16,\n",
        "    \"img_size\": 224,\n",
        "    \"output_size\": 128,\n",
        "    # Model\n",
        "    \"num_predictions\": 68,\n",
        "    \"base_features\": 64,\n",
        "    \"activation\": \"relu\",\n",
        "    \"batch_norm\": True,\n",
        "    # Training\n",
        "    \"lr\": 4e-3,\n",
        "    \"max_epochs\": 50,\n",
        "    \"criterion\": \"bce\",\n",
        "    \"random_seed\": 42,\n",
        "    \"patience\": 10,\n",
        "    \"optimizer\": \"adam\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the transformed dataset\n",
        "transformed_heatmap_dataset = FacialKeypointsHeatmapDataset(\n",
        "    csv_file=training_keypoints_csv_path,\n",
        "    root_dir=training_data_dir,\n",
        "    transform=data_transform,\n",
        "    output_size=config[\"output_size\"],\n",
        "    sigma=3.0,\n",
        ")\n",
        "\n",
        "# load training data in batches\n",
        "train_heatmap_loader = DataLoader(\n",
        "    transformed_heatmap_dataset,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")  # num_workers changed to Colab recommended number\n",
        "\n",
        "# creating the test dataset\n",
        "test_heatmap_dataset = FacialKeypointsHeatmapDataset(\n",
        "    csv_file=test_keypoints_csv_path,\n",
        "    root_dir=test_data_dir,\n",
        "    transform=data_transform,\n",
        "    output_size=config[\"output_size\"],\n",
        "    sigma=3.0,\n",
        ")\n",
        "\n",
        "# loading test data in batches\n",
        "test_heatmap_loader = DataLoader(\n",
        "    test_heatmap_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=2\n",
        ")  # num_workers changed to Colab recommended number, shuffle changed to False\n",
        "\n",
        "test_batch = next(iter(test_heatmap_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Heatmap Sanity Check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_hm = next(iter(train_heatmap_loader))[\"heatmaps\"]\n",
        "print(f\"Heatmap shape:       {sample_hm.shape}\")        # expect (B, 68, 128, 128)\n",
        "print(f\"Heatmap max:         {sample_hm.max():.4f}\")    # should be ~1.0\n",
        "print(f\"Heatmap mean:        {sample_hm.mean():.6f}\")   # should be very small\n",
        "print(f\"Fraction > 0.01:     {(sample_hm > 0.01).float().mean():.4f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "for i, ax in enumerate(axes):\n",
        "    im = ax.imshow(sample_hm[0, i * 20].numpy(), cmap=\"jet\")\n",
        "    ax.set_title(f\"GT heatmap kp {i * 20}\")\n",
        "    plt.colorbar(im, ax=ax)\n",
        "plt.suptitle(\"Ground-Truth Heatmaps \u2014 should show visible Gaussian peaks\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 UNet Overfit Test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# Architecture sanity check: model must nail a single batch within 50 steps.\n",
        "# Pass: loss well below 0.01. Fail: stuck above 0.05 \u2192 deeper issue.\n",
        "unet_overfit = UNetKeypointDetector(\n",
        "    num_predictions=config[\"num_predictions\"],\n",
        "    activation=config[\"activation\"],\n",
        "    base_features=config[\"base_features\"],\n",
        "    output_size=config[\"output_size\"],\n",
        ")\n",
        "keypoint_task_overfit = KeypointDetection(\n",
        "    model=unet_overfit,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    batch_norm=config[\"batch_norm\"],\n",
        ")\n",
        "trainer_overfit = Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator=\"auto\",\n",
        "    deterministic=\"warn\",\n",
        "    logger=False,\n",
        "    default_root_dir=default_exp_dir,\n",
        "    detect_anomaly=True,\n",
        "    overfit_batches=1,\n",
        "    enable_checkpointing=False,\n",
        ")\n",
        "trainer_overfit.fit(keypoint_task_overfit, train_dataloaders=train_heatmap_loader)\n",
        "metrics = trainer_overfit.callback_metrics\n",
        "print(f\"\\n=== Overfit Test ===\\nFinal loss: {metrics['train_loss']:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 0\n",
        "\n",
        "unet = UNetKeypointDetector(\n",
        "    num_predictions=config[\"num_predictions\"],\n",
        "    activation=config[\"activation\"],\n",
        "    base_features=config[\"base_features\"],\n",
        "    output_size=config[\"output_size\"],\n",
        ")\n",
        "\n",
        "keypoint_task = KeypointDetection(\n",
        "    model=unet,\n",
        "    lr=config[\"lr\"],\n",
        "    criterion=config[\"criterion\"],\n",
        "    patience=config[\"patience\"],\n",
        "    optimizer=config[\"optimizer\"],\n",
        "    pretrained_backbone=None,\n",
        "    dropout=None,\n",
        "    batch_norm=config[\"batch_norm\"],\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=default_exp_dir + f\"version_{version}\",\n",
        "    filename=\"unet\",\n",
        "    monitor=\"train_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        "    save_last=True,\n",
        "    save_weights_only=True,\n",
        "    enable_version_counter=True,\n",
        ")\n",
        "\n",
        "earlystopping_callback = EarlyStopping(\n",
        "    monitor=\"train_loss\",\n",
        "    patience=config[\"patience\"],\n",
        "    mode=\"min\",\n",
        "    min_delta=0.001,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    logger=[\n",
        "        TensorBoardLogger(\n",
        "            default_exp_dir,\n",
        "            name=\"\",  # necessary to save logs to the same directory as CSVLogger\n",
        "            version=f\"version_{version}\",\n",
        "            log_graph=True,\n",
        "            default_hp_metric=False,\n",
        "        ),\n",
        "        CSVLogger(default_exp_dir, name=\"\", version=f\"version_{version}\"),\n",
        "    ],\n",
        "    max_epochs=config[\"max_epochs\"],\n",
        "    callbacks=[checkpoint_callback, earlystopping_callback],\n",
        "    deterministic=\"warn\",\n",
        "    default_root_dir=default_exp_dir,\n",
        "    enable_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    keypoint_task,\n",
        "    train_dataloaders=train_heatmap_loader,\n",
        "    val_dataloaders=test_heatmap_loader,\n",
        ")\n",
        "\n",
        "keypoint_task = KeypointDetection.load_from_checkpoint(\n",
        "    checkpoint_callback.best_model_path, weights_only=True, model=unet\n",
        ")\n",
        "\n",
        "metrics = trainer.callback_metrics\n",
        "print(\n",
        "    f\"\\n\\n=============\\nFinal train loss: {metrics['train_loss']:.4f}\\nFinal val loss: {metrics['val_loss']:.4f}\"\n",
        ")\n",
        "\n",
        "visualize_heatmaps(unet, test_batch)\n",
        "visualize_loss_curve(\n",
        "    logs=f\"{default_exp_dir}version_{version}/metrics.csv\",\n",
        "    title=f\"Part 4: UNet Training Curve Version {version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {default_exp_dir}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
